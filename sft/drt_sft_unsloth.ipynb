{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "[NEW] Supports all Qwen 2.5 model sizes! 0.5, 1.5, 3, 7, 14, 32, 72b!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "184f373cce03454693e9a12fac2f06b7",
            "1cb15a73d14c45729672d225895b0b23",
            "2cccb0c2f00e4f369edcf640b5a3258e",
            "f37b9adcc7c3446bb073ae51a9c269aa",
            "60fb8cd9b18f42d292179a9451f91dae",
            "732ed170915142db84c3dc9dea000f68",
            "5f771b8b1b4f4d8fbebd52ab3edeae33",
            "b4a62f034f98489e820771510b2aff36",
            "98a35278e66d4ee0a63a8cc8ab0c1195",
            "136577da29174117a44c59cd17b126b5",
            "e847f03a15e24867b387629d15929a50"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "7e6c294c-4c4e-49a9-8745-7085abbf064c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.6.6: Fast Qwen2 patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "184f373cce03454693e9a12fac2f06b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    # Can select any from the below:\n",
        "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
        "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
        "    # And also all Instruct versions and Math. Coding verisons!\n",
        "    # model_name = \"unsloth/Qwen2.5-7B\",\n",
        "    model_name = \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"hf_\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gagZvnnYBdLZ",
        "outputId": "a4596e60-ad59-4b80-e670-cce99b285b45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'trans', 'thought'],\n",
              "        num_rows: 1300\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'trans', 'thought'],\n",
              "        num_rows: 338\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_file = \"/content/drt_train.jsonl\"\n",
        "val_file = \"/content/drt_val.jsonl\"\n",
        "\n",
        "# 📚 데이터셋 불러오기 (train / validation)\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": train_file, \"validation\": val_file})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255,
          "referenced_widgets": [
            "eb9279407a39480d8b785d6cd1a29cb5",
            "7f3191d65abe47e5bcf840c52c56b0fb",
            "8d093e1c6c174cc9ad3c738f7d645cab",
            "3b0f52247f934e32a1912f4a4166d4b4",
            "bc1b18cd667b4425b3b2e4008af670bd",
            "b1aade4ed69846ef9f412de2eee8b975",
            "c4f219e93b7d498e9e414e153f67d0ea",
            "677fe412607d49de99ec820946e4316a",
            "7c4c29ed0f134db4a1eadca1b6b7eda6",
            "1bf473af1430461aba6a488b04e8c5e0",
            "ec88c97f850d41f2be0b5e9776a1bca5",
            "87560d72c12843f39123a1e80ffcd73f",
            "b107dc7f3cb34a249c05152e3c0c1a44",
            "8e5cccab40c84664b8a454b981961de7",
            "7406a567615344e4b4daf4ed86dc6dec",
            "a063c967c30844228b9e99f29b0fcf44",
            "07d23ea7e0734fce9635d45aa39b4dd4",
            "d52bc74093144d7792c34246819bc56c",
            "ab2f945e9b0f49f8b6e08ef5372903bd",
            "f4b818f8f3bc4818b5ee028cc52809f9",
            "dcffadd9fd5d4068be1cc752c2363bbd",
            "d934a3ac43cc4b91aac00ee63a381145"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "966904a1-4721-4b51-8909-0fc7b5330d9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb9279407a39480d8b785d6cd1a29cb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1300 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87560d72c12843f39123a1e80ffcd73f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/338 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'trans', 'thought'],\n",
              "        num_rows: 1300\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'trans', 'thought'],\n",
              "        num_rows: 338\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 💬 Chat 형식 prompt 포맷팅 (Few-shot 예제 추가)\n",
        "def format_chat(example):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Please translate the following text from Korean to English:\\n{example['text']}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"<thought>{example['thought']}</thought><output>{example['trans']}</output>\"}\n",
        "    ]\n",
        "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
        "\n",
        "dataset = dataset.map(format_chat)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8915d088b6854d15bb8cc4d74fd5ccb8",
            "7fa2a79d2f7c428a9f920232fbe0f866",
            "c435b8c0f10d48b0b73613b7a4c65ea4",
            "e4abfa532a07409397ac687eabbeb223",
            "2b53d6c46a1c49baab1e2d7162544cc2",
            "a007ed28e34d444288b4e9e6f5ad780e",
            "cec1b4d355d14c8d875d1055bcfd7c09",
            "28788b0a75e74b72ac469b0f66a2970a",
            "53c51868cdb84b1ebadb8e2522cc1b13",
            "19b8323db77346a8b775883c0bfca674",
            "bd169522019a47c5beee5b5109305166",
            "3ddad856c0f04807890f557c3463dcb8",
            "e575511d914f42b687532e9b4d862283",
            "8c19ba784a434fc28bd5ba2e0ed8494d",
            "a0cdec441e17419c8875360a861eb588",
            "5388d54ea61d47afae809264b3618e9e",
            "98169758e84548b188a21b6693293628",
            "ea7b2bfb218d42f9b3fa4a199c813c43",
            "6f9dca36c03c4408914b83e5bec89394",
            "77dcef1e52f04b79bb4e0e8ef7d500bf",
            "97d81d22c3654a1ba337c841bc429100",
            "e6c97498c4b7466c9604db930cb44780"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "4f14cd23-0bf7-4017-d5cc-9bc3c0ead1df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8915d088b6854d15bb8cc4d74fd5ccb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/1300 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ddad856c0f04807890f557c3463dcb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/338 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset[\"train\"], # Use the train split of the loaded dataset\n",
        "    eval_dataset = dataset[\"validation\"], # Use the validation split\n",
        "    # dataset_text_field = \"text\", # Use the 'text' field from your dataset\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "d736edca-25e0-45f0-b426-ac2d4b6fcaca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "21.551 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "fa2fb0f1-c370-4251-bd95-2aa654a720df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,300 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 40,370,176/7,000,000,000 (0.58% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:52, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.995200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.132900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.020900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.959500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.995700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.615900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.375300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.371600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.289700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.183200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.185500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.241500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.032300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.042000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.967100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.935200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.879400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.859500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.876700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.969000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.872200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.810400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.769000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.774400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.856100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.792800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.819700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.820300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.814000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.915400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.832700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.679700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.799600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.718500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.726300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.789200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.753900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.770100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.737100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.745300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.769300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.766500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.772400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.749600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.723200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.677300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.687300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.789600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.736700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.714500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.722500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.757300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.805600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.746100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.837000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.808300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.773800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit/resolve/main/config.json (Request ID: Root=1-685d7b4b-5ee074d4207c51970933bced;77e39e2d-001b-4322-bf76-99d5291bd8c7)\n",
            "\n",
            "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "pCqnaKmlO1U9",
        "outputId": "63902e59-ffb9-4d0e-adf7-d8f0daafab4d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'start_gpu_memory' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-3481187506.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Show final memory and time stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mused_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_memory_reserved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mused_memory_for_lora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_gpu_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mused_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory\u001b[0m         \u001b[0;34m/\u001b[0m\u001b[0mmax_memory\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlora_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory_for_lora\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_memory\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'start_gpu_memory' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "7fa8ed8d-e04f-4dde-b375-be243050671d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|im_start|>system\\nYou are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight.<|im_end|>\\n<|im_start|>user\\nPlease translate the following text from Korean to English:\\n나는 과제를 하느라 완전히 파김치가 되었어.<|im_end|>\\n<|im_start|>assistant\\n<|im_end|>\\n<|im_start|>assistant\\n<thought>I start by selecting keywords from the Korean sentence: 과제 (task), 완전히 (completely), 파김치 (scalded rice cake).\\nI translate the keywords: 과제 (task), 완전히 (completely), 파김치 (scalded rice cake']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"system\", \"content\": \"You are a helpful Korean-English translator who provides detailed analysis before translation.\"},\n",
        "#     {\"role\": \"user\", \"content\": chat_template.format(\"나는 과제를 하느라 완전히 파김치가 되었어.\")},\n",
        "#     {\"role\": \"assistant\", \"content\": \"\"}, # Leave assistant content blank for generation\n",
        "# ]\n",
        "prompt = \"Please translate the following text from Korean to English:\\n나는 과제를 하느라 완전히 파김치가 되었어.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a philosopher skilled in deep thinking, accustomed to exploring complex problems with profound insight.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "    {\"role\": \"assistant\", \"content\": \"\"}, # Leave assistant content blank for generation\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens = 64, use_cache = True)\n",
        "outputs = tokenizer.batch_decode(outputs)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "2a6db85d-e793-4eb6-ca8f-989c46e50f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "You are a helpful Korean-English translator who provides detailed analysis before translation.<|im_end|>\n",
            "<|im_start|>user\n",
            "Please translate the following text from Korean to English:\n",
            "\n",
            "Here are examples of the expected format:\n",
            "\n",
            "Example 1:\n",
            "Korean: 식은 죽 먹기\n",
            "<thought>'식은 죽 먹기' is a Korean idiom that literally means 'eating cold porridge'. It refers to something that is very easy or simple to do, equivalent to the English idiom 'a piece of cake'. This expression is commonly used in Korean to describe tasks or situations that require minimal effort or skill.</thought>\n",
            "<output>A piece of cake</output>\n",
            "\n",
            "Example 2:\n",
            "Korean: 금강산도 식후경\n",
            "<thought>This is a Korean proverb meaning that even the most beautiful scenery is better appreciated after having a meal. It emphasizes the importance of taking care of basic needs first before enjoying other pleasures.</thought>\n",
            "<output>Even the beautiful Diamond Mountain is better after a meal</output>\n",
            "\n",
            "Now translate this:\n",
            "Korean: 안녕하세요<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<thought>I start by selecting the keywords from the Korean sentence: \"안녕하세요\" which translates to \"hello\" or \"hi\" in English.\n",
            "I then consider the context and cultural significance of the phrase, noting that it is commonly used as a greeting in Korean.\n",
            "I reflect on the potential nuances and variations in meaning depending on the situation or relationship between the speaker and listener.\n",
            "I make a decision to use the more formal and polite version of the greeting, \"Hello,\" for clarity and appropriateness in an English translation.\n",
            "I review my decision and feel confident that it effectively conveys the intended meaning while being natural and easily understood\n"
          ]
        }
      ],
      "source": [
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Define the chat template based on the format_chat function\n",
        "chat_template = \"\"\"Please translate the following text from Korean to English:\n",
        "\n",
        "Here are examples of the expected format:\n",
        "\n",
        "Example 1:\n",
        "Korean: 식은 죽 먹기\n",
        "<thought>'식은 죽 먹기' is a Korean idiom that literally means 'eating cold porridge'. It refers to something that is very easy or simple to do, equivalent to the English idiom 'a piece of cake'. This expression is commonly used in Korean to describe tasks or situations that require minimal effort or skill.</thought>\n",
        "<output>A piece of cake</output>\n",
        "\n",
        "Example 2:\n",
        "Korean: 금강산도 식후경\n",
        "<thought>This is a Korean proverb meaning that even the most beautiful scenery is better appreciated after having a meal. It emphasizes the importance of taking care of basic needs first before enjoying other pleasures.</thought>\n",
        "<output>Even the beautiful Diamond Mountain is better after a meal</output>\n",
        "\n",
        "Now translate this:\n",
        "Korean: {}\"\"\" # Placeholder for the input Korean text\n",
        "\n",
        "user_input = \"안녕하세요\" # Replace with your Korean text for inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful Korean-English translator who provides detailed analysis before translation.\"},\n",
        "    {\"role\": \"user\", \"content\": chat_template.format(user_input)},\n",
        "    {\"role\": \"assistant\", \"content\": \"\"}, # Leave assistant content blank for generation\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids=inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "0995f3eb-3eb9-423f-ede2-3b7f2f9f10c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/vocab.json',\n",
              " 'lora_model/merges.txt',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "0da36005-0a24-4b7d-ae01-083465d7acbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "You are a helpful Korean-English translator who provides detailed analysis before translation.<|im_end|>\n",
            "<|im_start|>user\n",
            "Please translate the following text from Korean to English:\n",
            "\n",
            "Here are examples of the expected format:\n",
            "\n",
            "Example 1:\n",
            "Korean: 식은 죽 먹기\n",
            "<thought>'식은 죽 먹기' is a Korean idiom that literally means 'eating cold porridge'. It refers to something that is very easy or simple to do, equivalent to the English idiom 'a piece of cake'. This expression is commonly used in Korean to describe tasks or situations that require minimal effort or skill.</thought>\n",
            "<output>A piece of cake</output>\n",
            "\n",
            "Example 2:\n",
            "Korean: 금강산도 식후경\n",
            "<thought>This is a Korean proverb meaning that even the most beautiful scenery is better appreciated after having a meal. It emphasizes the importance of taking care of basic needs first before enjoying other pleasures.</thought>\n",
            "<output>Even the beautiful Diamond Mountain is better after a meal</output>\n",
            "\n",
            "Now translate this:\n",
            "Korean: 안녕하세요<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Korean: 안녕하세요\n",
            "\n",
            "Thought: \"안녕하세요\" is a formal greeting in Korean, which is commonly used when meeting someone for the first time or in professional settings. It can be translated as \"Hello\" or \"Good day.\" The phrase is often accompanied by a slight bow and is used in various contexts to express a polite welcome or greeting.\n",
            "\n",
            "Output: Hello<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# Define the chat template based on the format_chat function\n",
        "chat_template = \"\"\"Please translate the following text from Korean to English:\n",
        "\n",
        "Here are examples of the expected format:\n",
        "\n",
        "Example 1:\n",
        "Korean: 식은 죽 먹기\n",
        "<thought>'식은 죽 먹기' is a Korean idiom that literally means 'eating cold porridge'. It refers to something that is very easy or simple to do, equivalent to the English idiom 'a piece of cake'. This expression is commonly used in Korean to describe tasks or situations that require minimal effort or skill.</thought>\n",
        "<output>A piece of cake</output>\n",
        "\n",
        "Example 2:\n",
        "Korean: 금강산도 식후경\n",
        "<thought>This is a Korean proverb meaning that even the most beautiful scenery is better appreciated after having a meal. It emphasizes the importance of taking care of basic needs first before enjoying other pleasures.</thought>\n",
        "<output>Even the beautiful Diamond Mountain is better after a meal</output>\n",
        "\n",
        "Now translate this:\n",
        "Korean: {}\"\"\" # Placeholder for the input Korean text\n",
        "\n",
        "\n",
        "user_input = \"안녕하세요\" # Replace with your Korean text for inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful Korean-English translator who provides detailed analysis before translation.\"},\n",
        "    {\"role\": \"user\", \"content\": chat_template.format(user_input)},\n",
        "    {\"role\": \"assistant\", \"content\": \"\"}, # Leave assistant content blank for generation\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids=inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqfebeAdT073",
        "outputId": "3b52932d-a30b-4167-aa68-b1a961474e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 56.81 out of 83.48 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:00<00:00, 61.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n",
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at model into bf16 GGUF format.\n",
            "The output location will be /content/model/unsloth.BF16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:hf-to-gguf:Model architecture: Qwen2ForCausalLM\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {3584, 152064}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {18944, 3584}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3584, 18944}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3584, 3584}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3584, 512}\n",
            "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3584}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> BF16, shape = {3584, 152064}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 32768\n",
            "INFO:hf-to-gguf:gguf: embedding length = 3584\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 18944\n",
            "INFO:hf-to-gguf:gguf: head count = 28\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 4\n",
            "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:gguf.vocab:Adding 151387 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type eos to 151645\n",
            "INFO:gguf.vocab:Setting special token type pad to 151654\n",
            "INFO:gguf.vocab:Setting add_bos_token to False\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/model/unsloth.BF16.gguf: n_tensors = 339, total_size = 15.2G\n",
            "Writing: 100%|██████████| 15.2G/15.2G [01:20<00:00, 190Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 5762 (a01047b0)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.BF16.gguf' to '/content/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 24 threads\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 339 tensors from /content/model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Model\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 7.6B\n",
            "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 18944\n",
            "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 28\n",
            "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type bf16:  198 tensors\n",
            "[   1/ 339]                        output.weight - [ 3584, 152064,     1,     1], type =   bf16, converting to q6_K .. size =  1039.50 MiB ->   426.36 MiB\n",
            "[   2/ 339]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   3/ 339]                    token_embd.weight - [ 3584, 152064,     1,     1], type =   bf16, converting to q4_K .. size =  1039.50 MiB ->   292.36 MiB\n",
            "[   4/ 339]                    blk.0.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[   5/ 339]                  blk.0.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[   6/ 339]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   7/ 339]             blk.0.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[   8/ 339]                    blk.0.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[   9/ 339]                  blk.0.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  10/ 339]                    blk.0.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  11/ 339]                  blk.0.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[  12/ 339]                blk.0.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[  13/ 339]                blk.0.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  14/ 339]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  15/ 339]                  blk.0.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  16/ 339]                    blk.1.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  17/ 339]                  blk.1.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  18/ 339]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  19/ 339]             blk.1.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  20/ 339]                    blk.1.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  21/ 339]                  blk.1.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  22/ 339]                    blk.1.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  23/ 339]                  blk.1.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[  24/ 339]                blk.1.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[  25/ 339]                blk.1.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  26/ 339]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  27/ 339]                  blk.1.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  28/ 339]                    blk.2.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  29/ 339]                  blk.2.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  30/ 339]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  31/ 339]             blk.2.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  32/ 339]                    blk.2.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  33/ 339]                  blk.2.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  34/ 339]                    blk.2.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  35/ 339]                  blk.2.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[  36/ 339]                blk.2.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[  37/ 339]                blk.2.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  38/ 339]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  39/ 339]                  blk.2.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  40/ 339]                    blk.3.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  41/ 339]                  blk.3.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  42/ 339]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  43/ 339]             blk.3.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  44/ 339]                    blk.3.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  45/ 339]                  blk.3.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  46/ 339]                    blk.3.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  47/ 339]                  blk.3.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  48/ 339]                blk.3.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  49/ 339]                blk.3.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  50/ 339]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  51/ 339]                  blk.3.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  52/ 339]                    blk.4.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  53/ 339]                  blk.4.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  54/ 339]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  55/ 339]             blk.4.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  56/ 339]                    blk.4.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  57/ 339]                  blk.4.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  58/ 339]                    blk.4.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  59/ 339]                  blk.4.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  60/ 339]                blk.4.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  61/ 339]                blk.4.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  62/ 339]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  63/ 339]                  blk.4.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  64/ 339]                    blk.5.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  65/ 339]                  blk.5.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  66/ 339]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  67/ 339]             blk.5.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  68/ 339]                    blk.5.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  69/ 339]                  blk.5.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  70/ 339]                    blk.5.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  71/ 339]                  blk.5.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[  72/ 339]                blk.5.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[  73/ 339]                blk.5.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  74/ 339]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  75/ 339]                  blk.5.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  76/ 339]                    blk.6.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  77/ 339]                  blk.6.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  78/ 339]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  79/ 339]             blk.6.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  80/ 339]                    blk.6.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  81/ 339]                  blk.6.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  82/ 339]                    blk.6.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  83/ 339]                  blk.6.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  84/ 339]                blk.6.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  85/ 339]                blk.6.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  86/ 339]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  87/ 339]                  blk.6.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  88/ 339]                    blk.7.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  89/ 339]                  blk.7.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  90/ 339]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  91/ 339]             blk.7.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  92/ 339]                    blk.7.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  93/ 339]                  blk.7.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[  94/ 339]                    blk.7.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[  95/ 339]                  blk.7.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[  96/ 339]                blk.7.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  97/ 339]                blk.7.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[  98/ 339]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[  99/ 339]                  blk.7.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 100/ 339]                    blk.8.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 101/ 339]                  blk.8.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 102/ 339]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 103/ 339]             blk.8.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 104/ 339]                    blk.8.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 105/ 339]                  blk.8.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 106/ 339]                    blk.8.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 107/ 339]                  blk.8.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 108/ 339]                blk.8.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 109/ 339]                blk.8.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 110/ 339]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 111/ 339]                  blk.8.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 112/ 339]                    blk.9.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 113/ 339]                  blk.9.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 114/ 339]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 115/ 339]             blk.9.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 116/ 339]                    blk.9.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 117/ 339]                  blk.9.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 118/ 339]                    blk.9.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 119/ 339]                  blk.9.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 120/ 339]                blk.9.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 121/ 339]                blk.9.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 122/ 339]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 123/ 339]                  blk.9.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 124/ 339]                   blk.10.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 125/ 339]                 blk.10.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 126/ 339]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 127/ 339]            blk.10.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 128/ 339]                   blk.10.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 129/ 339]                 blk.10.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 130/ 339]                   blk.10.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 131/ 339]                 blk.10.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 132/ 339]               blk.10.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 133/ 339]               blk.10.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 134/ 339]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 135/ 339]                 blk.10.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 136/ 339]                   blk.11.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 137/ 339]                 blk.11.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 138/ 339]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 139/ 339]            blk.11.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 140/ 339]                   blk.11.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 141/ 339]                 blk.11.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 142/ 339]                   blk.11.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 143/ 339]                 blk.11.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 144/ 339]               blk.11.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 145/ 339]               blk.11.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 146/ 339]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 147/ 339]                 blk.11.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 148/ 339]                   blk.12.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 149/ 339]                 blk.12.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 150/ 339]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 151/ 339]            blk.12.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 152/ 339]                   blk.12.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 153/ 339]                 blk.12.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 154/ 339]                   blk.12.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 155/ 339]                 blk.12.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 156/ 339]               blk.12.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 157/ 339]               blk.12.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 158/ 339]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 159/ 339]                 blk.12.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 160/ 339]                   blk.13.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 161/ 339]                 blk.13.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 162/ 339]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 163/ 339]            blk.13.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 164/ 339]                   blk.13.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 165/ 339]                 blk.13.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 166/ 339]                   blk.13.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 167/ 339]                 blk.13.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 168/ 339]               blk.13.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 169/ 339]               blk.13.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 170/ 339]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 171/ 339]                 blk.13.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 172/ 339]                   blk.14.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 173/ 339]                 blk.14.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 174/ 339]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 175/ 339]            blk.14.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 176/ 339]                   blk.14.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 177/ 339]                 blk.14.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 178/ 339]                   blk.14.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 179/ 339]                 blk.14.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 180/ 339]               blk.14.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 181/ 339]               blk.14.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 182/ 339]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 183/ 339]                 blk.14.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 184/ 339]                   blk.15.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 185/ 339]                 blk.15.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 186/ 339]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 187/ 339]            blk.15.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 188/ 339]                   blk.15.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 189/ 339]                 blk.15.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 190/ 339]                   blk.15.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 191/ 339]                 blk.15.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 192/ 339]               blk.15.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 193/ 339]               blk.15.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 194/ 339]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 195/ 339]                 blk.15.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 196/ 339]                   blk.16.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 197/ 339]                 blk.16.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 198/ 339]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 199/ 339]            blk.16.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 200/ 339]                   blk.16.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 201/ 339]                 blk.16.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 202/ 339]                   blk.16.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 203/ 339]                 blk.16.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 204/ 339]               blk.16.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 205/ 339]               blk.16.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 206/ 339]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 207/ 339]                 blk.16.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 208/ 339]                   blk.17.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 209/ 339]                 blk.17.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 210/ 339]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 211/ 339]            blk.17.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 212/ 339]                   blk.17.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 213/ 339]                 blk.17.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 214/ 339]                   blk.17.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 215/ 339]                 blk.17.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 216/ 339]               blk.17.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 217/ 339]               blk.17.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 218/ 339]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 219/ 339]                 blk.17.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 220/ 339]                   blk.18.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 221/ 339]                 blk.18.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 222/ 339]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 223/ 339]            blk.18.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 224/ 339]                   blk.18.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 225/ 339]                 blk.18.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 226/ 339]                   blk.18.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 227/ 339]                 blk.18.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 228/ 339]               blk.18.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 229/ 339]               blk.18.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 230/ 339]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 231/ 339]                 blk.18.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 232/ 339]                   blk.19.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 233/ 339]                 blk.19.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 234/ 339]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 235/ 339]            blk.19.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 236/ 339]                   blk.19.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 237/ 339]                 blk.19.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 238/ 339]                   blk.19.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 239/ 339]                 blk.19.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 240/ 339]               blk.19.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 241/ 339]               blk.19.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 242/ 339]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 243/ 339]                 blk.19.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 244/ 339]                   blk.20.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 245/ 339]                 blk.20.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 246/ 339]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 247/ 339]            blk.20.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 248/ 339]                   blk.20.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 249/ 339]                 blk.20.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 250/ 339]                   blk.20.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 251/ 339]                 blk.20.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 252/ 339]               blk.20.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 253/ 339]               blk.20.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 254/ 339]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 255/ 339]                 blk.20.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 256/ 339]                   blk.21.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 257/ 339]                 blk.21.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 258/ 339]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 259/ 339]            blk.21.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 260/ 339]                   blk.21.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 261/ 339]                 blk.21.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 262/ 339]                   blk.21.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 263/ 339]                 blk.21.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 264/ 339]               blk.21.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 265/ 339]               blk.21.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 266/ 339]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 267/ 339]                 blk.21.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 268/ 339]                   blk.22.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 269/ 339]                 blk.22.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 270/ 339]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 271/ 339]            blk.22.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 272/ 339]                   blk.22.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 273/ 339]                 blk.22.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 274/ 339]                   blk.22.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 275/ 339]                 blk.22.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 276/ 339]               blk.22.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 277/ 339]               blk.22.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 278/ 339]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 279/ 339]                 blk.22.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 280/ 339]                   blk.23.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 281/ 339]                 blk.23.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 282/ 339]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 283/ 339]            blk.23.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 284/ 339]                   blk.23.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 285/ 339]                 blk.23.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 286/ 339]                   blk.23.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 287/ 339]                 blk.23.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 288/ 339]               blk.23.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 289/ 339]               blk.23.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 290/ 339]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 291/ 339]                 blk.23.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 292/ 339]                   blk.24.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 293/ 339]                 blk.24.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 294/ 339]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 295/ 339]            blk.24.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 296/ 339]                   blk.24.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 297/ 339]                 blk.24.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 298/ 339]                   blk.24.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 299/ 339]                 blk.24.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 300/ 339]               blk.24.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 301/ 339]               blk.24.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 302/ 339]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 303/ 339]                 blk.24.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 304/ 339]                   blk.25.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 305/ 339]                 blk.25.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 306/ 339]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 307/ 339]            blk.25.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 308/ 339]                   blk.25.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 309/ 339]                 blk.25.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 310/ 339]                   blk.25.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 311/ 339]                 blk.25.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 312/ 339]               blk.25.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 313/ 339]               blk.25.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 314/ 339]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 315/ 339]                 blk.25.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 316/ 339]                   blk.26.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 317/ 339]                 blk.26.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 318/ 339]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 319/ 339]            blk.26.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 320/ 339]                   blk.26.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 321/ 339]                 blk.26.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 322/ 339]                   blk.26.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 323/ 339]                 blk.26.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 324/ 339]               blk.26.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 325/ 339]               blk.26.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 326/ 339]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 327/ 339]                 blk.26.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 328/ 339]                   blk.27.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 329/ 339]                 blk.27.attn_k.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
            "[ 330/ 339]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 331/ 339]            blk.27.attn_output.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 332/ 339]                   blk.27.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 333/ 339]                 blk.27.attn_q.weight - [ 3584,  3584,     1,     1], type =   bf16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
            "[ 334/ 339]                   blk.27.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
            "[ 335/ 339]                 blk.27.attn_v.weight - [ 3584,   512,     1,     1], type =   bf16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
            "[ 336/ 339]               blk.27.ffn_down.weight - [18944,  3584,     1,     1], type =   bf16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
            "[ 337/ 339]               blk.27.ffn_gate.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "[ 338/ 339]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
            "[ 339/ 339]                 blk.27.ffn_up.weight - [ 3584, 18944,     1,     1], type =   bf16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
            "llama_model_quantize_impl: model size  = 14526.27 MB\n",
            "llama_model_quantize_impl: quant size  =  4460.45 MB\n",
            "\n",
            "main: quantize time = 128744.21 ms\n",
            "main:    total time = 128744.21 ms\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"JudyChoi/drt\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"JudyChoi/drt\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"JudyChoi/drt\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"JudyChoi/drt\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"hf_\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html).\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗 HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "10. [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "11. [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "12. [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYHpPyccO-Ww"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "e5030bde",
        "outputId": "f4144f44-3fce-49d5-afd8-0a73b7cea87b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_81f1c69a-3493-4e97-b32b-a9c31b1a640f\", \"unsloth.Q4_K_M.gguf\", 4683070752)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/model/unsloth.Q4_K_M.gguf')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07d23ea7e0734fce9635d45aa39b4dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "136577da29174117a44c59cd17b126b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "184f373cce03454693e9a12fac2f06b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cb15a73d14c45729672d225895b0b23",
              "IPY_MODEL_2cccb0c2f00e4f369edcf640b5a3258e",
              "IPY_MODEL_f37b9adcc7c3446bb073ae51a9c269aa"
            ],
            "layout": "IPY_MODEL_60fb8cd9b18f42d292179a9451f91dae"
          }
        },
        "19b8323db77346a8b775883c0bfca674": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bf473af1430461aba6a488b04e8c5e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cb15a73d14c45729672d225895b0b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_732ed170915142db84c3dc9dea000f68",
            "placeholder": "​",
            "style": "IPY_MODEL_5f771b8b1b4f4d8fbebd52ab3edeae33",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "28788b0a75e74b72ac469b0f66a2970a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b53d6c46a1c49baab1e2d7162544cc2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cccb0c2f00e4f369edcf640b5a3258e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4a62f034f98489e820771510b2aff36",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98a35278e66d4ee0a63a8cc8ab0c1195",
            "value": 2
          }
        },
        "3b0f52247f934e32a1912f4a4166d4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bf473af1430461aba6a488b04e8c5e0",
            "placeholder": "​",
            "style": "IPY_MODEL_ec88c97f850d41f2be0b5e9776a1bca5",
            "value": " 1300/1300 [00:00&lt;00:00, 5672.11 examples/s]"
          }
        },
        "3ddad856c0f04807890f557c3463dcb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e575511d914f42b687532e9b4d862283",
              "IPY_MODEL_8c19ba784a434fc28bd5ba2e0ed8494d",
              "IPY_MODEL_a0cdec441e17419c8875360a861eb588"
            ],
            "layout": "IPY_MODEL_5388d54ea61d47afae809264b3618e9e"
          }
        },
        "5388d54ea61d47afae809264b3618e9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53c51868cdb84b1ebadb8e2522cc1b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f771b8b1b4f4d8fbebd52ab3edeae33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60fb8cd9b18f42d292179a9451f91dae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "677fe412607d49de99ec820946e4316a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f9dca36c03c4408914b83e5bec89394": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "732ed170915142db84c3dc9dea000f68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7406a567615344e4b4daf4ed86dc6dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcffadd9fd5d4068be1cc752c2363bbd",
            "placeholder": "​",
            "style": "IPY_MODEL_d934a3ac43cc4b91aac00ee63a381145",
            "value": " 338/338 [00:00&lt;00:00, 4905.33 examples/s]"
          }
        },
        "77dcef1e52f04b79bb4e0e8ef7d500bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c4c29ed0f134db4a1eadca1b6b7eda6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f3191d65abe47e5bcf840c52c56b0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1aade4ed69846ef9f412de2eee8b975",
            "placeholder": "​",
            "style": "IPY_MODEL_c4f219e93b7d498e9e414e153f67d0ea",
            "value": "Map: 100%"
          }
        },
        "7fa2a79d2f7c428a9f920232fbe0f866": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a007ed28e34d444288b4e9e6f5ad780e",
            "placeholder": "​",
            "style": "IPY_MODEL_cec1b4d355d14c8d875d1055bcfd7c09",
            "value": "Unsloth: Tokenizing [&quot;text&quot;]: 100%"
          }
        },
        "87560d72c12843f39123a1e80ffcd73f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b107dc7f3cb34a249c05152e3c0c1a44",
              "IPY_MODEL_8e5cccab40c84664b8a454b981961de7",
              "IPY_MODEL_7406a567615344e4b4daf4ed86dc6dec"
            ],
            "layout": "IPY_MODEL_a063c967c30844228b9e99f29b0fcf44"
          }
        },
        "8915d088b6854d15bb8cc4d74fd5ccb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fa2a79d2f7c428a9f920232fbe0f866",
              "IPY_MODEL_c435b8c0f10d48b0b73613b7a4c65ea4",
              "IPY_MODEL_e4abfa532a07409397ac687eabbeb223"
            ],
            "layout": "IPY_MODEL_2b53d6c46a1c49baab1e2d7162544cc2"
          }
        },
        "8c19ba784a434fc28bd5ba2e0ed8494d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f9dca36c03c4408914b83e5bec89394",
            "max": 338,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77dcef1e52f04b79bb4e0e8ef7d500bf",
            "value": 338
          }
        },
        "8d093e1c6c174cc9ad3c738f7d645cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_677fe412607d49de99ec820946e4316a",
            "max": 1300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c4c29ed0f134db4a1eadca1b6b7eda6",
            "value": 1300
          }
        },
        "8e5cccab40c84664b8a454b981961de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab2f945e9b0f49f8b6e08ef5372903bd",
            "max": 338,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4b818f8f3bc4818b5ee028cc52809f9",
            "value": 338
          }
        },
        "97d81d22c3654a1ba337c841bc429100": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98169758e84548b188a21b6693293628": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a35278e66d4ee0a63a8cc8ab0c1195": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a007ed28e34d444288b4e9e6f5ad780e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a063c967c30844228b9e99f29b0fcf44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0cdec441e17419c8875360a861eb588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97d81d22c3654a1ba337c841bc429100",
            "placeholder": "​",
            "style": "IPY_MODEL_e6c97498c4b7466c9604db930cb44780",
            "value": " 338/338 [00:00&lt;00:00, 2516.10 examples/s]"
          }
        },
        "ab2f945e9b0f49f8b6e08ef5372903bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b107dc7f3cb34a249c05152e3c0c1a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07d23ea7e0734fce9635d45aa39b4dd4",
            "placeholder": "​",
            "style": "IPY_MODEL_d52bc74093144d7792c34246819bc56c",
            "value": "Map: 100%"
          }
        },
        "b1aade4ed69846ef9f412de2eee8b975": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a62f034f98489e820771510b2aff36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc1b18cd667b4425b3b2e4008af670bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd169522019a47c5beee5b5109305166": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c435b8c0f10d48b0b73613b7a4c65ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28788b0a75e74b72ac469b0f66a2970a",
            "max": 1300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53c51868cdb84b1ebadb8e2522cc1b13",
            "value": 1300
          }
        },
        "c4f219e93b7d498e9e414e153f67d0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cec1b4d355d14c8d875d1055bcfd7c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d52bc74093144d7792c34246819bc56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d934a3ac43cc4b91aac00ee63a381145": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcffadd9fd5d4068be1cc752c2363bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4abfa532a07409397ac687eabbeb223": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19b8323db77346a8b775883c0bfca674",
            "placeholder": "​",
            "style": "IPY_MODEL_bd169522019a47c5beee5b5109305166",
            "value": " 1300/1300 [00:00&lt;00:00, 2613.51 examples/s]"
          }
        },
        "e575511d914f42b687532e9b4d862283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98169758e84548b188a21b6693293628",
            "placeholder": "​",
            "style": "IPY_MODEL_ea7b2bfb218d42f9b3fa4a199c813c43",
            "value": "Unsloth: Tokenizing [&quot;text&quot;]: 100%"
          }
        },
        "e6c97498c4b7466c9604db930cb44780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e847f03a15e24867b387629d15929a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea7b2bfb218d42f9b3fa4a199c813c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb9279407a39480d8b785d6cd1a29cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f3191d65abe47e5bcf840c52c56b0fb",
              "IPY_MODEL_8d093e1c6c174cc9ad3c738f7d645cab",
              "IPY_MODEL_3b0f52247f934e32a1912f4a4166d4b4"
            ],
            "layout": "IPY_MODEL_bc1b18cd667b4425b3b2e4008af670bd"
          }
        },
        "ec88c97f850d41f2be0b5e9776a1bca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f37b9adcc7c3446bb073ae51a9c269aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_136577da29174117a44c59cd17b126b5",
            "placeholder": "​",
            "style": "IPY_MODEL_e847f03a15e24867b387629d15929a50",
            "value": " 2/2 [00:02&lt;00:00,  1.16s/it]"
          }
        },
        "f4b818f8f3bc4818b5ee028cc52809f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
