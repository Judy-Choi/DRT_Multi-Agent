"""
ì„¤ì • ë¡œë” ë° LLM ìƒì„±
"""
import tomllib
import logging
from pathlib import Path
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from openai import RateLimitError
from anthropic import RateLimitError as AnthropicRateLimitError
from langchain_core.runnables import Runnable

logger = logging.getLogger(__name__)

def load_settings():
    """settings.toml íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    config_path = Path(__file__).parent / "settings.toml"
    
    with open(config_path, "rb") as f:
        return tomllib.load(f)

def _create_llm_instance(model_name, temperature, max_tokens):
    """ë‹¨ì¼ LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if "claude" in model_name:
        return ChatAnthropic(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens
        )
    elif "gpt" in model_name:
        return ChatOpenAI(
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens
        )
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model_name}")

def create_llm(settings=None):
    """ì„¤ì •ì— ë”°ë¼ LLMì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if settings is None:
        settings = load_settings()
    
    llm_config = settings["llm"]
    primary_model = llm_config["model"]
    fallback_model = llm_config.get("fallback_model")
    temperature = llm_config.get("temperature", 0.0)
    max_tokens = llm_config.get("max_tokens", 1024)

    # Fallback ê¸°ëŠ¥ì´ ìˆëŠ” LLM ë˜í¼ ë°˜í™˜
    return FallbackLLM(
        primary_model=primary_model,
        fallback_model=fallback_model,
        temperature=temperature,
        max_tokens=max_tokens
    )

class FallbackLLM(Runnable):
    """Fallback ê¸°ëŠ¥ì´ ìˆëŠ” LLM ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self, primary_model, fallback_model, temperature, max_tokens):
        self.primary_model = primary_model
        self.fallback_model = fallback_model
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # ì£¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.primary_llm = _create_llm_instance(primary_model, temperature, max_tokens)
        
        # ëŒ€ì²´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìˆëŠ” ê²½ìš°)
        self.fallback_llm = None
        if fallback_model:
            self.fallback_llm = _create_llm_instance(fallback_model, temperature, max_tokens)
    
    def invoke(self, messages, config=None):
        """ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ê³  fallback ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."""
        try:
            # ì£¼ ëª¨ë¸ë¡œ ì‹œë„
            return self.primary_llm.invoke(messages, config=config)
        except (RateLimitError, AnthropicRateLimitError) as e:
            if self.fallback_llm:
                logger.warning(f"ğŸ”„ {self.primary_model}ì—ì„œ ì†ë„ ì œí•œ ë°œìƒ, {self.fallback_model}ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return self.fallback_llm.invoke(messages, config=config)
            else:
                logger.error("âŒ Fallback ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ì˜¤ë¥˜ë¥¼ ì¬ë°œìƒì‹œí‚µë‹ˆë‹¤.")
                raise
        except Exception as e:
            # ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì˜¤ë¥˜ë„ fallbackìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
            if self.fallback_llm and "rate" in str(e).lower():
                logger.warning(f"ğŸ”„ {self.primary_model}ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ({e}), {self.fallback_model}ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return self.fallback_llm.invoke(messages, config=config)
            else:
                raise
    
    def __getattr__(self, name):
        """ë‹¤ë¥¸ ë©”ì„œë“œë“¤ì€ ì£¼ ëª¨ë¸ë¡œ ìœ„ì„"""
        return getattr(self.primary_llm, name)

# ì„¤ì • ë¡œë“œ
SETTINGS = load_settings()

# ìì£¼ ì‚¬ìš©ë˜ëŠ” ê°’ë“¤ì„ ìƒìˆ˜ë¡œ ë…¸ì¶œ
MAX_ITERATIONS = SETTINGS["translation"]["max_iterations"]
LLM_MODEL = SETTINGS["llm"]["model"] 